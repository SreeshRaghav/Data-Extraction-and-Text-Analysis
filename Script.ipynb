{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNT90BUD3BbZ3gb6AQW82PM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GZm1Uw8RQ8BZ"},"outputs":[],"source":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UhvjVFxRiVv","executionInfo":{"status":"ok","timestamp":1725860099748,"user_tz":-330,"elapsed":67318,"user":{"displayName":"CJ","userId":"03750305435780555654"}},"outputId":"78c6cef8-62a7-4d10-b1f2-9e56e99c310a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import re\n","\n","# Read the URL file into the pandas object\n","input_df = pd.read_excel('/content/drive/MyDrive/Blackcoffer/Input.xlsx')\n","\n","# Create the directory for text files if it doesn't exist\n","content_folder = '/content/drive/MyDrive/Blackcoffer/TextTitle/'\n","os.makedirs(content_folder, exist_ok=True)\n","\n","# Loop through each row in the dataframe\n","for idx, row in input_df.iterrows():\n","    web_link = row['URL']\n","    page_id = row['URL_ID']\n","\n","    # Make a request to the URL\n","    user_agent = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n","    try:\n","        web_response = requests.get(web_link, headers=user_agent)\n","    except:\n","        print(f\"Failed to fetch response for {page_id}\")\n","        continue\n","\n","    # Create a BeautifulSoup object\n","    try:\n","        page_content = BeautifulSoup(web_response.content, 'html.parser')\n","    except:\n","        print(f\"Failed to load page for {page_id}\")\n","        continue\n","\n","    # Extract title\n","    try:\n","        page_title = page_content.find('h1').get_text()\n","    except:\n","        print(f\"Failed to extract title for {page_id}\")\n","        continue\n","\n","    # Extract text content\n","    text_content = \"\"\n","    try:\n","        for para in page_content.find_all('p'):\n","            text_content += para.get_text()\n","    except:\n","        print(f\"Failed to extract text for {page_id}\")\n","        continue\n","\n","    # Save title and text to a file\n","    output_file = os.path.join(content_folder, f'{page_id}.txt')\n","    with open(output_file, 'w') as file:\n","        file.write(page_title + '\\n' + text_content)\n","\n","# Directories\n","text_content_dir = \"/content/drive/MyDrive/Blackcoffer/TextTitle/\"\n","stopword_path = \"/content/drive/MyDrive/Blackcoffer/StopWords\"\n","sentiment_path = \"/content/drive/MyDrive/Blackcoffer/MasterDictionary\"\n","\n","# Load stopwords from directory and store in a set\n","custom_stopwords = set()\n","for stopword_file in os.listdir(stopword_path):\n","    with open(os.path.join(stopword_path, stopword_file), 'r', encoding='ISO-8859-1') as f:\n","        custom_stopwords.update(set(f.read().splitlines()))\n","\n","# Load all text files from the directory and store them in a list (cleaned_docs)\n","cleaned_docs = []\n","for doc_file in os.listdir(text_content_dir):\n","    with open(os.path.join(text_content_dir, doc_file), 'r') as f:\n","        doc_text = f.read()\n","        # Tokenize the text\n","        tokens = word_tokenize(doc_text)\n","        # Remove stopwords from tokens\n","        filtered_words = [word for word in tokens if word.lower() not in custom_stopwords]\n","        # Add filtered tokens to the list\n","        cleaned_docs.append(filtered_words)\n","\n","# Load positive and negative words from the sentiment dictionary\n","positive_words_set = set()\n","negative_words_set = set()\n","\n","for dict_file in os.listdir(sentiment_path):\n","    if dict_file == 'positive-words.txt':\n","        with open(os.path.join(sentiment_path, dict_file), 'r', encoding='ISO-8859-1') as f:\n","            positive_words_set.update(f.read().splitlines())\n","    elif dict_file == 'negative-words.txt':\n","        with open(os.path.join(sentiment_path, dict_file), 'r', encoding='ISO-8859-1') as f:\n","            negative_words_set.update(f.read().splitlines())\n","\n","# Analyze positive and negative words from each document\n","pos_words_list = []\n","neg_words_list = []\n","positive_scores = []\n","negative_scores = []\n","polarity_scores = []\n","subjectivity_scores = []\n","\n","for doc in cleaned_docs:\n","    pos_words_list.append([word for word in doc if word.lower() in positive_words_set])\n","    neg_words_list.append([word for word in doc if word.lower() in negative_words_set])\n","    positive_scores.append(len(pos_words_list[-1]))\n","    negative_scores.append(len(neg_words_list[-1]))\n","    polarity_scores.append((positive_scores[-1] - negative_scores[-1]) / ((positive_scores[-1] + negative_scores[-1]) + 0.000001))\n","    subjectivity_scores.append((positive_scores[-1] + negative_scores[-1]) / (len(doc) + 0.000001))\n","\n","# Function to calculate readability metrics\n","def calculate_readability_metrics(doc_file):\n","    with open(os.path.join(text_content_dir, doc_file), 'r') as f:\n","        doc_text = f.read()\n","        doc_text = re.sub(r'[^\\w\\s.]', '', doc_text)\n","        sentences = doc_text.split('.')\n","        total_sentences = len(sentences)\n","        words_in_doc = [word for word in doc_text.split() if word.lower() not in custom_stopwords]\n","        total_words = len(words_in_doc)\n","\n","        # Identify complex words (with more than 2 syllables)\n","        complex_words = []\n","        for word in words_in_doc:\n","            syllable_count = sum(1 for letter in word if letter.lower() in 'aeiou')\n","            if syllable_count > 2:\n","                complex_words.append(word)\n","\n","        # Calculate average syllables per word\n","        syllable_sum = 0\n","        syllable_word_list = []\n","        for word in words_in_doc:\n","            syllable_count = sum(1 for letter in word if letter.lower() in 'aeiou')\n","            if syllable_count >= 1:\n","                syllable_word_list.append(word)\n","                syllable_sum += syllable_count\n","\n","        avg_sentence_length = total_words / total_sentences\n","        avg_syllables_per_word = syllable_sum / len(syllable_word_list)\n","        percent_complex_words = len(complex_words) / total_words\n","        fog_score = 0.4 * (avg_sentence_length + percent_complex_words)\n","\n","        return avg_sentence_length, percent_complex_words, fog_score, len(complex_words), avg_syllables_per_word\n","\n","# Calculate metrics for each document\n","avg_sentence_lengths = []\n","complex_word_percentages = []\n","fog_indexes = []\n","complex_word_counts = []\n","syllable_per_word_avg = []\n","\n","for doc_file in os.listdir(text_content_dir):\n","    a, b, c, d, e = calculate_readability_metrics(doc_file)\n","    avg_sentence_lengths.append(a)\n","    complex_word_percentages.append(b)\n","    fog_indexes.append(c)\n","    complex_word_counts.append(d)\n","    syllable_per_word_avg.append(e)\n","\n","# Function to clean and calculate word count and average word length\n","def count_cleaned_words(doc_file):\n","    with open(os.path.join(text_content_dir, doc_file), 'r') as f:\n","        doc_text = f.read()\n","        doc_text = re.sub(r'[^\\w\\s]', '', doc_text)\n","        words = [word for word in doc_text.split() if word.lower() not in custom_stopwords]\n","        total_characters = sum(len(word) for word in words)\n","        avg_word_length = total_characters / len(words)\n","    return len(words), avg_word_length\n","\n","total_word_count = []\n","avg_word_lengths = []\n","\n","for doc_file in os.listdir(text_content_dir):\n","    wc, awl = count_cleaned_words(doc_file)\n","    total_word_count.append(wc)\n","    avg_word_lengths.append(awl)\n","\n","# Function to count personal pronouns\n","def find_personal_pronouns(doc_file):\n","    with open(os.path.join(text_content_dir, doc_file), 'r') as f:\n","        doc_text = f.read()\n","        personal_pronouns_list = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","        pronoun_count = 0\n","        for pronoun in personal_pronouns_list:\n","            pronoun_count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", doc_text))\n","    return pronoun_count\n","\n","pronoun_counts = []\n","\n","for doc_file in os.listdir(text_content_dir):\n","    pronoun_counts.append(find_personal_pronouns(doc_file))\n","\n","# Load output DataFrame\n","output_df = pd.read_excel('/content/drive/MyDrive/Blackcoffer/Output Data Structure.xlsx')\n","\n","# These are the required variables\n","metrics_list = [\n","    positive_scores, negative_scores, polarity_scores, subjectivity_scores,\n","    avg_sentence_lengths, complex_word_percentages, fog_indexes,\n","    avg_sentence_lengths, complex_word_counts, total_word_count, syllable_per_word_avg,\n","    pronoun_counts, avg_word_lengths\n","]\n","\n","# Write the values to the output dataframe\n","for i, metric in enumerate(metrics_list):\n","    if len(metric) == len(output_df):\n","        output_df.iloc[:, i+2] = metric\n","    else:\n","        print(f\"Error: Length mismatch for metric {i+2}\")\n","\n","# Save the final output\n","output_df.to_csv('/content/drive/MyDrive/Blackcoffer/processed_output.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_02Dqu2PRzOp","executionInfo":{"status":"ok","timestamp":1725860815504,"user_tz":-330,"elapsed":588857,"user":{"displayName":"CJ","userId":"03750305435780555654"}},"outputId":"b19c61a8-12d8-4516-ca75-7d3f6c09da83"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]}]}